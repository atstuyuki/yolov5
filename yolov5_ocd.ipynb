{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c988e5f-5af1-4bd0-ab14-c0e25089ac89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\atsuy\\\\Documents'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a770a6e8-3345-49f4-ba6d-ae732c41ee19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\atsuy\\yolov5\n"
     ]
    }
   ],
   "source": [
    "#yolov5ディレクトリにいろいろ関数がおかれているのでこのディレクトリで操作する\n",
    "%cd ../yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd324965-cece-4e6b-86a6-8b50e33c9b2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=yolov5s.pt, source=0, data=data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
      "YOLOv5  v6.1-36-gc09fb2a torch 1.11.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "1/1: 0...  Success (inf frames 640x480 at 30.00 FPS)\n",
      "\n",
      "0: 480x640 1 person, Done. (0.225s)\n",
      "0: 480x640 1 person, Done. (0.209s)\n",
      "0: 480x640 1 person, Done. (0.209s)\n",
      "0: 480x640 1 person, Done. (0.205s)\n",
      "0: 480x640 1 person, Done. (0.198s)\n",
      "0: 480x640 1 person, Done. (0.206s)\n",
      "0: 480x640 1 person, Done. (0.192s)\n",
      "0: 480x640 1 person, Done. (0.183s)\n",
      "0: 480x640 1 person, Done. (0.183s)\n",
      "0: 480x640 1 person, Done. (0.193s)\n",
      "0: 480x640 1 person, Done. (0.206s)\n",
      "0: 480x640 1 person, Done. (0.206s)\n",
      "0: 480x640 1 person, Done. (0.190s)\n",
      "0: 480x640 1 person, Done. (0.199s)\n",
      "0: 480x640 1 person, Done. (0.191s)\n",
      "0: 480x640 1 person, Done. (0.192s)\n",
      "0: 480x640 1 person, Done. (0.190s)\n",
      "0: 480x640 1 person, Done. (0.190s)\n",
      "0: 480x640 1 person, Done. (0.185s)\n",
      "0: 480x640 1 person, Done. (0.187s)\n",
      "0: 480x640 1 person, Done. (0.183s)\n",
      "0: 480x640 1 person, Done. (0.189s)\n",
      "0: 480x640 1 person, Done. (0.192s)\n",
      "0: 480x640 1 person, Done. (0.192s)\n",
      "0: 480x640 1 person, Done. (0.189s)\n",
      "0: 480x640 1 person, Done. (0.192s)\n",
      "0: 480x640 1 person, Done. (0.189s)\n",
      "0: 480x640 1 person, Done. (0.199s)\n",
      "0: 480x640 1 person, Done. (0.184s)\n",
      "0: 480x640 1 person, Done. (0.190s)\n",
      "Speed: 0.1ms pre-process, 194.6ms inference, 1.4ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\exp119\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#yolov5　cocoモデルで80種類の物体を検出　”ｑ”押下で終了\n",
    "!python detect.py --source 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d18909-b247-4efb-a567-1ae566e55f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v6.1-36-gc09fb2a torch 1.11.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "#tourch hubを使って学習済みモデルをmodelに格納\n",
    "import torch\n",
    "#cocomodelを読み込む場合\n",
    "model = torch.hub.load(\"../yolov5\",\"yolov5s\",source='local')\n",
    "#ocd検出モデルを読み込む場合\n",
    "#model = torch.hub.load('','custom',path='ocd_20220413.pt',source='local',force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34497356-568b-4af3-a547-c1ffc35ad9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([], size=(0, 6))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nplt.imshow(results)\\nplt.show()'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#静止画で検出\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "imgs=cv2.imread('./data/images/ocd_1.jpg')\n",
    "results = model(imgs)\n",
    "print(results.xyxy[0])\n",
    "results.show()\n",
    "\"\"\"\n",
    "plt.imshow(results)\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa7e31c-f1ff-469f-9802-b3e3d5dcfdc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\atsuy\\Documents\\yolov5_ocd.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atsuy/Documents/yolov5_ocd.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m#画面を10*10分割して1/10のところから9/10のところまで切り抜き\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atsuy/Documents/yolov5_ocd.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     im_cropped\u001b[39m=\u001b[39mimgs[\u001b[39mround\u001b[39m(h\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m):\u001b[39mround\u001b[39m(h\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m9\u001b[39m, \u001b[39mround\u001b[39m(w\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m):\u001b[39mround\u001b[39m(w\u001b[39m/\u001b[39m\u001b[39m10\u001b[39m)\u001b[39m*\u001b[39m\u001b[39m9\u001b[39m,:]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/atsuy/Documents/yolov5_ocd.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     results \u001b[39m=\u001b[39m model(im_cropped)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atsuy/Documents/yolov5_ocd.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m \u001b[39m*\u001b[39mbox, conf, \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m results\u001b[39m.\u001b[39mxyxy[\u001b[39m0\u001b[39m]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/atsuy/Documents/yolov5_ocd.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         s \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mnames[\u001b[39mint\u001b[39m(\u001b[39mcls\u001b[39m)]\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{:.1f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mfloat\u001b[39m(conf)\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\yolov5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\yolov5\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\atsuy\\yolov5\\../yolov5\\models\\common.py:560\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[1;34m(self, imgs, size, augment, profile)\u001b[0m\n\u001b[0;32m    556\u001b[0m t\u001b[39m.\u001b[39mappend(time_sync())\n\u001b[0;32m    558\u001b[0m \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mautocast(enabled\u001b[39m=\u001b[39mautocast):\n\u001b[0;32m    559\u001b[0m     \u001b[39m# Inference\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x, augment, profile)  \u001b[39m# forward\u001b[39;00m\n\u001b[0;32m    561\u001b[0m     t\u001b[39m.\u001b[39mappend(time_sync())\n\u001b[0;32m    563\u001b[0m     \u001b[39m# Post-process\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\yolov5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\atsuy\\yolov5\\../yolov5\\models\\common.py:408\u001b[0m, in \u001b[0;36mDetectMultiBackend.forward\u001b[1;34m(self, im, augment, visualize, val)\u001b[0m\n\u001b[0;32m    406\u001b[0m b, ch, h, w \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mshape  \u001b[39m# batch, channel, height, width\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpt \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 408\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im, augment\u001b[39m=\u001b[39;49maugment, visualize\u001b[39m=\u001b[39;49mvisualize)\n\u001b[0;32m    409\u001b[0m     \u001b[39mreturn\u001b[39;00m y \u001b[39mif\u001b[39;00m val \u001b[39melse\u001b[39;00m y[\u001b[39m0\u001b[39m]\n\u001b[0;32m    410\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdnn:  \u001b[39m# ONNX OpenCV DNN\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\yolov5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\atsuy\\yolov5\\../yolov5\\models\\yolo.py:126\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[0;32m    125\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_augment(x)  \u001b[39m# augmented inference, None\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_once(x, profile, visualize)\n",
      "File \u001b[1;32mc:\\Users\\atsuy\\yolov5\\../yolov5\\models\\yolo.py:149\u001b[0m, in \u001b[0;36mModel._forward_once\u001b[1;34m(self, x, profile, visualize)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[0;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 149\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[0;32m    150\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\yolov5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\atsuy\\yolov5\\../yolov5\\models\\common.py:139\u001b[0m, in \u001b[0;36mC3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv3(torch\u001b[39m.\u001b[39;49mcat((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mm(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv1(x)), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv2(x)), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\yolov5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\atsuy\\yolov5\\../yolov5\\models\\common.py:50\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_fuse\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x))\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\yolov5\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\yolov5\\lib\\site-packages\\torch\\nn\\modules\\activation.py:394\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 394\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49msilu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\yolov5\\lib\\site-packages\\torch\\nn\\functional.py:2031\u001b[0m, in \u001b[0;36msilu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   2029\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m, inplace\u001b[39m=\u001b[39minplace)\n\u001b[0;32m   2030\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m-> 2031\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49msilu_(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m   2032\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39msilu(\u001b[39minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#webカメラで検出\n",
    "import cv2\n",
    "camera = cv2.VideoCapture(0) #外部入力カメラを使用する場合0を1に変更\n",
    "#camera = cv2.VideoCapture('./data/images/ocd.mp4')\n",
    "while True:\n",
    "    ret, imgs = camera.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    h,w,ch=imgs.shape\n",
    "#画面を10*10分割して1/10のところから9/10のところまで切り抜き\n",
    "    im_cropped=imgs[round(h/10):round(h/10)*9, round(w/10):round(w/10)*9,:]\n",
    "    results = model(im_cropped)\n",
    "    for *box, conf, cls in results.xyxy[0]:\n",
    "        s = model.names[int(cls)]+\":\"+'{:.1f}'.format(float(conf)*100)\n",
    "        cv2.rectangle(im_cropped,\n",
    "                      (int(box[0]),int(box[1])),\n",
    "                      (int(box[2]),int(box[3])),color=(0,255,0),thickness=4)\n",
    "        cv2.putText(im_cropped, s, (int(box[0]),int(box[1])-5), fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale=1.0,\n",
    "                   color=(0,0,255),thickness =2)\n",
    "    cv2.imshow('tfcc',imgs)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "camera.release()    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cc14e3",
   "metadata": {},
   "source": [
    "#GUIのレイアウト設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a8fb35a-7fc5-4a1a-8415-055f5da2aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PySimpleGUI as sg\n",
    "sg.theme('SystemDefault')\n",
    "values = [0, 1, 2]#cameraの番号設定\n",
    "layout = [\n",
    "   [sg.Image(key='img1',), sg.Image(key='img2',)],\n",
    "   [[sg.Text('select camera'),sg.Listbox(values, size=(10, 5))]],\n",
    "    [[sg.Button('Start', size=(10, 1)), sg.Button('Stop', size=(10,1))]],\n",
    "    [sg.Button('Exit', size=(10, 1))],]\n",
    "#記録イベントのためにstartedという関数を定義               \n",
    "started= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb663e51-1594-4679-b937-c97a5de17c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v6.1-36-gc09fb2a torch 1.11.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "#tourch hubを使って学習済みモデルをmodelに格納\n",
    "import torch\n",
    "#cocomodelを読み込む場合\n",
    "model = torch.hub.load(\"../yolov5\",\"yolov5s\",source='local')\n",
    "#ocd検出モデルを読み込む場合\n",
    "#model = torch.hub.load('','custom',path='ocd_20220413.pt',source='local',force_reload=True)\n",
    "\n",
    "#torchhubモデルの検出時のパラメーター設定\n",
    "\"\"\"model.conf=0.4\n",
    "model.iou=0.45\n",
    "model.multi_label=False\n",
    "model.max_det=5\"\"\"\n",
    "\n",
    "import PySimpleGUI as sg\n",
    "#すべての警告の非表示\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "#value = [0, 1, 2]#cameraの番号設定\n",
    "threshould=0.4 #閾値の初期設定\n",
    "#カメラ番号入力のポップアップ 外部入力なら1か2\n",
    "cameranum = sg.popup_get_text(\"input camera number(0,1,2)\", title=\"camera number\", default_text=\"1\")\n",
    "cameranum =int(cameranum)\n",
    "#GUIの初期設定\n",
    "sg.theme('DarkBlue15')\n",
    "layout = [\n",
    "   [sg.Image(key='img1',), sg.Image(key='img2',)],\n",
    "   #[[sg.Text('select camera'),sg.Listbox(value, size=(10, 3),key=('cameranum'))]],\n",
    "   [[sg.Text('confidence thredshold'),sg.Slider(range=(0.1,1.0),default_value=0.4, resolution=0.1 ,orientation='h',\n",
    "   size=(20,2),enable_events=True, key='slider',)]],\n",
    "    [[sg.Button('Start', size=(10, 1)), sg.Button('Stop', size=(10,1))]],\n",
    "\n",
    "    [sg.Button('Exit', size=(10, 1))],]\n",
    "#記録イベントのためにstartedという関数を定義               \n",
    "started= False\n",
    "#webカメラで検出\n",
    "import cv2\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "#camera = cv2.VideoCapture('./data/images/ocd.mp4')\n",
    "#pysimpleGUIのwindow定義\n",
    "window = sg.Window(\"webカメラ画面\", location=(100,100),layout=layout, size=(1000,1000),resizable=True,finalize=True)\n",
    "event, values = window.read(timeout=20)\n",
    "#cameranum = int(values['cameranum'])\n",
    "#if event == ['caneranum']:\n",
    "#    cameranum=int(values['cameranum'])\n",
    "cap = cv2.VideoCapture(cameranum) #外部入力カメラを使用する場合0を1に変更\n",
    "# ビデオ記録用の変数定義\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "out = cv2.VideoWriter('result_{}.mp4'.format(dt.datetime.now()), fourcc, fps, (w, h))\n",
    "while True:\n",
    "    ret, imgs = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    h,w,ch=imgs.shape\n",
    "# キャプチャーした画像をpngに変換\n",
    "    image1 = cv2.imencode('.png', imgs)[1].tobytes()                   \n",
    "# Imageの内容を更新\n",
    "    window['img1'].update(data=image1)#web cameraの画像を直接表示\n",
    "    event, values = window.read(timeout=0)\n",
    "    \"\"\"if event == ['caneranum']:\n",
    "        cameranum=int(values['cameranum'])\n",
    "        cap=cv2.VideoCapture(cameranum)\n",
    "        ret,img = cap.read()\"\"\"\n",
    "\n",
    "    if values['slider']:\n",
    "      threshould=values['slider']\n",
    "      model.conf=threshould\n",
    "\n",
    "    if event == 'Start':\n",
    "        started = True\n",
    "    if started == True:\n",
    "        #画面を10*10分割して1/10のところから9/10のところまで切り抜き\n",
    "        im_cropped=imgs[round(h/10):round(h/10)*9, round(w/10):round(w/10)*9,:]\n",
    "        results = model(im_cropped)\n",
    "        for *box, conf, cls in results.xyxy[0]:\n",
    "            s = model.names[int(cls)]+\":\"+'{:.1f}'.format(float(conf)*100)\n",
    "            cv2.rectangle(im_cropped,\n",
    "                      (int(box[0]),int(box[1])),\n",
    "                      (int(box[2]),int(box[3])),color=(0,255,0),thickness=4)\n",
    "            cv2.putText(im_cropped, s, (int(box[0]),int(box[1])-5), fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale=1.0,\n",
    "                   color=(0,0,255),thickness =2)          \n",
    "    # キャプチャーした画像をpngに変換\n",
    "            image2 = cv2.imencode('.png', im_cropped)[1].tobytes()\n",
    "    # Imageの内容を更新\n",
    "            window['img2'].update(data=image2)       \n",
    "\n",
    "    if event =='Stop':\n",
    "        started = False\n",
    "        out.release()\n",
    "\n",
    "    if event == 'Exit' or event == sg.WIN_CLOSED:\n",
    "        break\n",
    "\n",
    "cap.release()    \n",
    "window.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16694d-e3ee-4fdd-87d6-41ca66d129b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v6.1-36-gc09fb2a torch 1.11.0 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 213 layers, 7023610 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#tourch hubを使って学習済みモデルをmodelに格納\n",
    "import torch\n",
    "#cocomodelを読み込む場合\n",
    "#model = torch.hub.load(\"../yolov5\",\"yolov5s\",source='local')\n",
    "#ocd検出モデルを読み込む場合\n",
    "model = torch.hub.load('','custom',path='ocd_20220413.pt',source='local',force_reload=True)\n",
    "\n",
    "#torchhubモデルの検出時のパラメーター設定\n",
    "\"\"\"model.conf=0.4\n",
    "model.iou=0.45\n",
    "model.multi_label=False\n",
    "model.max_det=5\"\"\"\n",
    "\n",
    "import PySimpleGUI as sg\n",
    "#すべての警告の非表示\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "#value = [0, 1, 2]#cameraの番号設定\n",
    "threshould=0.4 #閾値の初期設定\n",
    "#カメラ番号入力のポップアップ 外部入力なら1か2\n",
    "cameranum = sg.popup_get_text(\"input camera number(0,1,2)\", title=\"camera number\", default_text=\"1\")\n",
    "cameranum =int(cameranum)\n",
    "#GUIの初期設定\n",
    "sg.theme('DarkBlue15')\n",
    "layout = [\n",
    "   [sg.Image(key='img1',), sg.Image(key='img2',)],\n",
    "   #[[sg.Text('select camera'),sg.Listbox(value, size=(10, 3),key=('cameranum'))]],\n",
    "   [[sg.Text('confidence thredshold'),sg.Slider(range=(0.1,1.0),default_value=0.4, resolution=0.1 ,orientation='h',\n",
    "   size=(20,2),enable_events=True, key='slider',)]],\n",
    "    [[sg.Button('Start', size=(10, 1)), sg.Button('Stop', size=(10,1))]],\n",
    "\n",
    "    [sg.Button('Exit', size=(10, 1))],]\n",
    "#記録イベントのためにstartedという関数を定義               \n",
    "started= False\n",
    "#webカメラで検出\n",
    "import cv2\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "#camera = cv2.VideoCapture('./data/images/ocd.mp4')\n",
    "#pysimpleGUIのwindow定義\n",
    "window = sg.Window(\"webカメラ画面\", location=(100,100),layout=layout, size=(1000,1000),resizable=True,finalize=True)\n",
    "event, values = window.read(timeout=20)\n",
    "#webカメラで検出\n",
    "import cv2\n",
    "import time\n",
    "import datetime as dt\n",
    "cap = cv2.VideoCapture(0) #外部入力カメラを使用する場合0を1に変更\n",
    "# ビデオ記録用の変数定義\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "out = cv2.VideoWriter('result_{}.mp4'.format(dt.datetime.now()), fourcc, fps, (w, h))\n",
    "#camera = cv2.VideoCapture('./data/images/ocd.mp4')\n",
    "#pysimpleGUIのwindow定義\n",
    "window = sg.Window(\"webカメラ画面\", location=(100,100),layout=layout, size=(1000,1000),resizable=True,finalize=True)\n",
    "while True:\n",
    "    ret, imgs = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    h,w,ch=imgs.shape\n",
    "#画面を10*10分割して1/10のところから9/10のところまで切り抜き\n",
    "    im_cropped=imgs[round(h/10):round(h/10)*9, round(w/10):round(w/10)*9,:]\n",
    "    results = model(im_cropped)\n",
    "    for *box, conf, cls in results.xyxy[0]:\n",
    "        s = model.names[int(cls)]+\":\"+'{:.1f}'.format(float(conf)*100)\n",
    "        cv2.rectangle(im_cropped,\n",
    "                      (int(box[0]),int(box[1])),\n",
    "                      (int(box[2]),int(box[3])),color=(0,255,0),thickness=4)\n",
    "        cv2.putText(im_cropped, s, (int(box[0]),int(box[1])-5), fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale=1.0,\n",
    "                   color=(0,0,255),thickness =2)\n",
    "# キャプチャーした画像をpngに変換\n",
    "    image1 = cv2.imencode('.png', imgs)[1].tobytes()                   \n",
    "# Imageの内容を更新\n",
    "    window['img1'].update(data=image1)#web cameraの画像を直接表示\n",
    "    event, values = window.read(timeout=0)\n",
    "    if event == 'Start YOLOV5':\n",
    "        started = True\n",
    "            \n",
    "    if started == True:\n",
    "    #画像を録画用に保存\n",
    "            out.write(im_cropped)                   \n",
    "    # キャプチャーした画像をpngに変換\n",
    "            image2 = cv2.imencode('.png', im_cropped)[1].tobytes()\n",
    "    # Imageの内容を更新\n",
    "            window['img2'].update(data=image2)       \n",
    "\n",
    "    if event =='Stop Log':\n",
    "            started = False\n",
    "            out.release()\n",
    "\n",
    "    if event == 'Exit' or event == sg.WIN_CLOSED:\n",
    "            break\n",
    "\n",
    "camera.release()    \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf3771-dbe5-444d-a5c5-05b541526b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no results\n"
     ]
    }
   ],
   "source": [
    "if results == True:\n",
    "     print('yes')\n",
    "else:\n",
    "    print('no results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce648c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b10d776fb881c9d02a4e5c1778afffc150285caed7c601fd6a068b4b09f96cf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
